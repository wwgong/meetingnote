# Design of Early Distribution for Multi-Partition Transaction #

### Transaction Queue ###

  * 1. Add a new queue "m\_multiPartitionReadyTxnQueue" containing all transactions already distributed fragments and ready to run its work units

  * 2. Old Queue "m\_transactionQueue" still contains all incoming transactions haven't yet been process, and "m\_currentTransactionState" still be the transaction which is running its work units

  * 3. Instead of fetching one transaction each time from "m\_transactionQueue" to "m\_currentTransactionState", we will put transactions to "m\_multiPartitionReadyTxnQueue" when it's ready first, and then when its turn to run, assign it to "m\_currentTransactionState"

#### Priority ####

If there is any ready to start txn in "m\_transactionQueue", we will retrieve it and distribute its fragments even though there is any ready to run txn in "m\_multiPartitionReadyTxnQueue".


## New Process for One-Shot Multi-Partition Transaction ##
  * Ready to start -- transaction is not blocked in "m\_transactionQueue"
  * Next to run -- transaction is the first one in "m\_multiPartitionReadyTxnQueue"

  * 1. Execution Site (as the coordinator) sees a transaction is ready to start, pull it out from "m\_transactionQueue".

  * 2. Execution Site (as the coordinator) initializes the transaction (func: initiateProcedure()), which process the initiate task message first time (func: distributeFragmentForMultiPartitionTask()).

  * 3. Execution Site forks a thread and stores it in our thread queue.

  * 4. The new thread calls the VoltProcedure corresponding to current transaction trying to get the result

  * 5. Execution Site (as the coordinator) puts this transaction to "m\_multiPartitionReadyTxnQueue", and at the same time, the new thread creates all fragments and sends them out.

  * 6. After distributing fragments, the new thread is waiting to get result back from the coordinator site.

  * 7. When the transaction is ready to run, and after the Execution Site (as the coordinator) finish the dispatching work (func: createAllParticipatingFragmentWork()), it knows the distributing phase has finished and it's safe to continue.

  * 8. It sends itself a fake message to let the Execution Site to resume work.

  * 9. Execution Site (as the coordinator) starts to process all fragments and after all works, the result stored in the "m\_previousStackFrameDropDependencies" in MultiPartitionParticipantTransactionState.

  * 10. The new thread sees "m\_previousStackFrameDropDependencies" is not null, it retrieves results and returns wrapped result.

  * 11. Execution Site receives the wrapped result and return it to the client end.

## For Multi-Shot Multi-Partition Transaction ##

  * 11. After step 10 in above process, the new thread will enter another "voltExecuteSQL()" run, and calls "slowPath()", so another fake message will be issued by the coordinator and sent to itself.

  * 12. The Coordinator executes all work units and prepare the result in the "m\_previousStackFrameDropDependencies".

  * 13. After the new result is stored, the new thread will see it, it retrieves the result and returns the wrapped result.

  * 14. After finishing all shots, Execution Site will eventually receives the wrapped result and returns to client.

## Source code ##

  * Available at escrow/20111121-frontend.zip.





# Escrow operations #
| prior - later | Escrow Update | Escrow Insert | Escrow Delete | Read |
|:--------------|:--------------|:--------------|:--------------|:-----|
| Escrow Update | "overdraft" | N/A | remove | wait |
| Escrow Insert | check | check & wait | check & remove | wait |
| Escrow Delete | check & wait | check & mark | check | wait |
| Read | N/A | N/A | N/A | N/A |



  * Escrow Update - Escrow Update: for multi-shot, consider if one shot succeeds, then another update occurs which cause later shot of previous txn fails, based on the txn id order, let previous txn prepare first and put unsuccessful txn in the waiting list

  * Escrow Update - Escrow Delete: to delete a row been updating, remove the escrow journal associated with the update and do the delete

  * Escrow Insert - Escrow Update: need to check whether we need to update any data in the journal are newly inserted but not committed yet, if so, apply modification on them as well

  * Escrow Insert - Escrow Insert: check inserting data whether any inserted but un-committed row in escrow journal, is so, wait until previous ones commits or abort

  * Escrow Insert - Escrow Delete: check whether deleting data containing any inserted but un-committed row in escrow journal, if so, to commit the delta txn need to remove the journal and already inserted data added by the un-committed insert txn

  * Escrow Delete - Escrow Update:  postpone the update if it needs to update data involved in previous delete txn

  * Escrow Delete - Escrow Insert: it's legal to insert a row which is deleted in previous txn, so mark the delete escrow journal if later escrow insert occurs

  * Escrow Delete - Escrow Delete: two escrow delete journal are not allowed existing in the escrow journal at the same time


## Structure supporting Escrow Operation ##

Similar to the lock manager,  on each site,

  * for each escrow data item,
    * a linked list representing the order of operations (escrow ops and read) applied on the data item

  * for each transaction
    * a linked list representing the order of operations in the txn

  * when a block (escrow journal) is added to this system, always checked with existing journals which might conflicts with it, and do the marking
    * there are three marks: conflict with insert, conflict with delete, conflict with update
    * if no mark, this block is free to move forward in the data item list
    * if any mark found, the block should stop when it sees the first conflict block while moving it forward